services:

  backend:
    image: rag_it/backend:latest
    build:
      context: ./services/backend
      dockerfile: Dockerfile
    container_name: rag_it_backend
    ports:
      - "8000:8000"
    volumes:
      - ./services/backend/src:/app/src
      - ./data/uploads:/app/uploads
      - ./data/processing:/app/processing
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - OLLAMA_SERVICE_URL=http://ollama:11434
      - EMBEDDING_SERVICE_URL=http://embeddings:8001
      - UNSTRUCTURED_SERVICE_URL=http://unstructured:8002
    depends_on:
      - qdrant
      - ollama
      - embeddings
      - unstructured
    restart: always
    networks:
      - rag_network

  frontend:
    image: rag_it/frontend:latest
    build:
      context: ./services/frontend
      dockerfile: Dockerfile
    container_name: rag_it_frontend
    ports:
      - "8501:8501"
    environment:
      - BACKEND_URL=http://backend:8000
      - APP_TITLE=RAG Chatbot
    depends_on:
      - backend
    volumes:
      - ./services/frontend/app.py:/app/app.py
    restart: always
    networks:
      - rag_network

  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    container_name: rag_it_qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./services/qdrant/config.yaml:/qdrant/config/production.yaml
      - ./data/qdrant:/qdrant/storage
    networks:
      - rag_network

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ./data/ollama:/root/.ollama
    restart: always
    container_name: rag_it_ollama
    ports:
      - "11434:11434"
    # Default Model - WIRD DIREKT HERUNTERGELADEN ZU BEGINN
    entrypoint: sh
    command: >
          -c "ollama serve & 
              sleep 5 && 
              ollama pull llama3:8b && 
              wait"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - rag_network

  embeddings:
    image: rag_it/embeddings:latest
    build:
      context: ./services/embeddings
      dockerfile: Dockerfile
    container_name: rag_it_embeddings
    ports:
      - "8001:8001"
    environment:
      - EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: always
    networks:
      - rag_network
      
  unstructured:
    image: rag_it/unstructured:latest
    build:
      context: ./services/unstructured
      dockerfile: Dockerfile
    container_name: rag_it_unstructured
    ports:
      - "8002:8002"
    restart: always
    networks:
      - rag_network

networks:
  rag_network:
    driver: bridge